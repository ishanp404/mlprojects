{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "19PCWzucIWb6ZvPghpEaFkQnfdwpBphcJ",
      "authorship_tag": "ABX9TyMUSioE+56K2k4Mwl43czCo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishanp404/mlprojects/blob/main/Road_Sign_Detection_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1RlPgK1PwQWT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "import cv2\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "from keras.layers import Dropout, Conv2D, MaxPool2D, Dense, Activation, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_xml_annotation(xml_file_path):\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "    annotations = []\n",
        "    for object_elem in root.findall('object'):\n",
        "        obj_info = {\n",
        "            'class': object_elem.find('name').text,\n",
        "            'xmin': int(object_elem.find('bndbox/xmin').text),\n",
        "            'ymin': int(object_elem.find('bndbox/ymin').text),\n",
        "            'xmax': int(object_elem.find('bndbox/xmax').text),\n",
        "            'ymax': int(object_elem.find('bndbox/ymax').text),\n",
        "        }\n",
        "        annotations.append(obj_info)\n",
        "    return annotations\n"
      ],
      "metadata": {
        "id": "uP9pKkEE1dKi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_image(image_path, target_size):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, target_size)\n",
        "    image = image / 255.0\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "P2JDxo-m9iRw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_directory = '/content/drive/MyDrive/Road Sign/images'\n",
        "annotation_directory = '/content/drive/MyDrive/Road Sign/annotations'\n",
        "\n",
        "image_files = [f for f in os.listdir(image_directory) if f.endswith('.png')]"
      ],
      "metadata": {
        "id": "syWlNqtT-9zd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_annotation_pairs = []\n",
        "for image_filename in image_files:\n",
        "    image_path = os.path.join(image_directory, image_filename)\n",
        "    annotation_path = os.path.join(annotation_directory, image_filename.replace('.png', '.xml'))\n",
        "\n",
        "    if os.path.exists(annotation_path):\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "        annotations = []\n",
        "        for object_elem in root.findall('object'):\n",
        "            obj_info = {\n",
        "                'class': object_elem.find('name').text,\n",
        "                'xmin': int(object_elem.find('bndbox/xmin').text),\n",
        "                'ymin': int(object_elem.find('bndbox/ymin').text),\n",
        "                'xmax': int(object_elem.find('bndbox/xmax').text),\n",
        "                'ymax': int(object_elem.find('bndbox/ymax').text),\n",
        "            }\n",
        "            annotations.append(obj_info)\n",
        "\n",
        "        image_annotation_pairs.append({'image': image, 'annotations': annotations})"
      ],
      "metadata": {
        "id": "HqJ9de_Q8bC5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_mapping = {\n",
        "    'trafficlight': 0,\n",
        "    'stop': 1,\n",
        "    'speedlimit': 2,\n",
        "    'crosswalk': 3\n",
        "}\n",
        "\n",
        "for data_point in image_annotation_pairs:\n",
        "    annotations = data_point['annotations']\n",
        "    for annotation in annotations:\n",
        "        class_name = annotation['class']\n",
        "        if class_name in class_mapping:\n",
        "            annotation['class'] = class_mapping[class_name]\n",
        "        else:\n",
        "            annotation['class'] = -1\n",
        "    annotations = data_point['annotations']\n",
        "    class_labels = [annotation['class'] for annotation in annotations]\n",
        "    data_point['annotations'] = class_labels[0]"
      ],
      "metadata": {
        "id": "bXckmlOm7-ZK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "random.shuffle(image_annotation_pairs)\n",
        "\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(image_annotation_pairs) * split_ratio)\n",
        "\n",
        "training_data = image_annotation_pairs[:split_index]\n",
        "testing_data = image_annotation_pairs[split_index:]"
      ],
      "metadata": {
        "id": "Z3AsqUUw-Pli"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = [data_point['image'] for data_point in training_data]\n",
        "y_train = [data_point['annotations'] for data_point in training_data]\n",
        "\n",
        "x_test = [data_point['image'] for data_point in testing_data]\n",
        "y_test = [data_point['annotations'] for data_point in testing_data]\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train, dtype=np.int64)\n",
        "y_test = np.array(y_test, dtype=np.int64)\n",
        "\n",
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0"
      ],
      "metadata": {
        "id": "LRL-5zjk6xTv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eBk9x7micCGA",
        "outputId": "22ee84b4-27e4-4720-cac7-20b42e5b5f1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(701, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GBcHRZv7cD4o",
        "outputId": "e6becdb1-feca-4ed9-ac36-e207b27d86b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(176, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_URL = 'https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5'"
      ],
      "metadata": {
        "id": "p2KWfivWmiBj"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = (None, 224, 224, 3)"
      ],
      "metadata": {
        "id": "gQkkUdLmoPpX"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, MaxPool2D, Dense, Activation, Dropout, Flatten\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "model = Sequential([\n",
        "    hub.KerasLayer(MODEL_URL)\n",
        "])\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=4, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"sparse_categorical_accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "S3_Wxxk1F7zd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=['sparse_categorical_crossentropy'], metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "8FQ2ybknTZxC"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.build(input_shape=INPUT_SHAPE)"
      ],
      "metadata": {
        "id": "ANfOl61ZqYtz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmipU_9Ym_Kx",
        "outputId": "9c7d473e-9cd2-467e-be25-541df3e0b453"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_15 (KerasLayer  (None, 1001)              5432713   \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 1001)              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 4)                 4008      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5436721 (20.74 MB)\n",
            "Trainable params: 4008 (15.66 KB)\n",
            "Non-trainable params: 5432713 (20.72 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "ec = EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "lHTjC4cAHE6g"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=30, callbacks=[ec])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIyhU74PIyPO",
        "outputId": "1bce54ef-804b-4d53-8493-61afa31fd6c2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "22/22 [==============================] - 17s 193ms/step - loss: 0.8576 - accuracy: 0.7247 - val_loss: 0.6239 - val_accuracy: 0.7670\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 2s 69ms/step - loss: 0.4457 - accuracy: 0.8545 - val_loss: 0.4813 - val_accuracy: 0.7955\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 2s 70ms/step - loss: 0.3093 - accuracy: 0.9001 - val_loss: 0.4253 - val_accuracy: 0.8409\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 2s 71ms/step - loss: 0.2418 - accuracy: 0.9173 - val_loss: 0.4137 - val_accuracy: 0.8523\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 2s 91ms/step - loss: 0.2075 - accuracy: 0.9401 - val_loss: 0.4268 - val_accuracy: 0.8523\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 0.1781 - accuracy: 0.9401 - val_loss: 0.3965 - val_accuracy: 0.8636\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 2s 71ms/step - loss: 0.1563 - accuracy: 0.9572 - val_loss: 0.4004 - val_accuracy: 0.8693\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 2s 69ms/step - loss: 0.1412 - accuracy: 0.9615 - val_loss: 0.4045 - val_accuracy: 0.8693\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 2s 69ms/step - loss: 0.1249 - accuracy: 0.9686 - val_loss: 0.4106 - val_accuracy: 0.8864\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 2s 69ms/step - loss: 0.1158 - accuracy: 0.9729 - val_loss: 0.4209 - val_accuracy: 0.8636\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 2s 71ms/step - loss: 0.1076 - accuracy: 0.9757 - val_loss: 0.4177 - val_accuracy: 0.8807\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 2s 69ms/step - loss: 0.0999 - accuracy: 0.9772 - val_loss: 0.4295 - val_accuracy: 0.8807\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 2s 69ms/step - loss: 0.0942 - accuracy: 0.9800 - val_loss: 0.4358 - val_accuracy: 0.8693\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 0.0883 - accuracy: 0.9757 - val_loss: 0.4382 - val_accuracy: 0.8750\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 2s 77ms/step - loss: 0.0804 - accuracy: 0.9843 - val_loss: 0.4468 - val_accuracy: 0.8750\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 2s 71ms/step - loss: 0.0774 - accuracy: 0.9829 - val_loss: 0.4556 - val_accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e2d49972260>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReyC3qecJZJV",
        "outputId": "f820dd7f-6301-428c-f035-e7f81f25ce7d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 60ms/step - loss: 0.4556 - accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.45562323927879333, 0.875]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cM45vK_IB0er"
      },
      "execution_count": 78,
      "outputs": []
    }
  ]
}